\documentclass{article}
\usepackage{pgf,tikz,tikzscale} 
\usepackage{amssymb}
\usepackage{tcolorbox}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{multicol}
\usepackage{enumerate}	
\usepackage{graphicx,lipsum,pgfplots} 
\usepackage{amsmath, amsthm}                 
\usepackage[top=1in,bottom=1in, left=.5in, right=.5in] {geometry}  
\usepackage{fancyhdr}       

\newcommand{\Var}{\operatorname{Var}} %% \Var{X}
\newcommand{\Cov}{\operatorname{Cov}}

\pagestyle{fancy}              
\lhead{Stat 5930 \newline Homework Chapter 2}   
\rhead{Warren Keil}






\begin{document}
\setlength{\parindent}{0cm}   %%%%%%%% KEEP THIS  for block style para. 



%%%%%%%%%%%%%%%%%        1a   %%%%%%%%%%%%%%%%%%%%
\textbf{2b.} 

\textbf{3. } Suppose the \(Y   \sim N_3(\mu, \Sigma), \), where 
\[
\mu=\begin{bmatrix}
2\\1\\2
\end{bmatrix}
\text{         and           } 
\Sigma = \begin{bmatrix}
2 & 1 & 1 \\ 
1 & 3 &0 \\
1 & 0 & 1
\end{bmatrix}.
\]
Find the joint distribution of \(Z_1 = Y_1 + Y_2 + Y_3 \) and \(Z_2 = Y_1-Y_2\). 

\vspace{4mm}
\textit{Solution. } 
Notice \( Z = \begin{bmatrix}
1&1&1\\
1&-1&0 
\end{bmatrix}\begin{bmatrix}
Y_1\\
Y_2\\
Y_3
\end{bmatrix}=CY
\).
By theorem 2.2, \(Z \sim N_2(C\mu+d, C\Sigma C')\).  And notice \(C\mu=\begin{bmatrix}
5\\
1
\end{bmatrix}\) and 

\begin{align*}
C\Sigma C' &= \begin{bmatrix}
1&1&1\\
1&-1&0 
\end{bmatrix} \begin{bmatrix}
2 & 1 & 1 \\ 
1 & 3 &0 \\
1 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
1&1\\
1& -1\\
1&0
\end{bmatrix}\\
&= \begin{bmatrix}
4& 4& 2\\
1 & -2 & 1 
\end{bmatrix}\begin{bmatrix}
1&1\\
1&-1\\
1&0
\end{bmatrix}\\
&= \begin{bmatrix}
10 & 0\\
0 & 3
\end{bmatrix}.
\end{align*}
\begin{flushright}
\(\blacksquare\)
\end{flushright} 

\newpage
\textbf{6.} If \(Y_1, Y_2\) are random variables such that \(Y_1+Y_2\) and \(Y_1-Y_2\) are independent \(N(0,1)\) random variables, show that \(Y_1\) and \( Y_2\) have a bivariate normal distribution and find its mean and variance. 


\vspace{4mm}
\textit{Solution. } Let \(X = X_1, X_2\) where \(X_1 = Y_1+Y_2\) and \(X_2=Y_1 - Y_2\) . This means that \(Y_1 = X_1 -Y_2\), and \(Y_2 = Y_1-X_2\). Thus \(Y_1 = .5X_1 + .5X_2\) and \(Y_2 = .5X_1 -.5X_2\) . Thus by theorem 2.2, 
\[
 Y \sim N_2(C\mu, C\Sigma C')  \text{ with }C=\begin{bmatrix}
 1/2 & 1/2\\
 1/2 & -1/2
 \end{bmatrix}
\]
And 
\[
C\mu = C * [0,0]'
\]
and
\begin{align*}
C\Sigma C' &= \begin{bmatrix}
 1/2 & 1/2\\
 1/2 & -1/2
 \end{bmatrix}
 \begin{bmatrix}
 1 & 0\\
 0& 1
 \end{bmatrix}
 \begin{bmatrix}
 1/2 & 1/2\\
 1/2 & -1/2
 \end{bmatrix} \\
 &= 
 \begin{bmatrix}
 1/2 & 1/2\\
 1/2 & -1/2
 \end{bmatrix}
 \begin{bmatrix}
 1/2 & 1/2\\
 1/2 & -1/2
 \end{bmatrix} \\
 &= \begin{bmatrix}
 1/2 & 0\\
 0 & 1/2 
 \end{bmatrix}
\end{align*}


\begin{flushright}
\(\blacksquare\)
\end{flushright} 

\newpage
\textbf{9. } Let \(X_1,..., X_3\) be iid \(N(0,1)\). Let \(Y = AX\) where \[
A= \begin{bmatrix}
\frac1{\sqrt{3 }} & \frac1{\sqrt{ 3}}&\frac1{\sqrt{ 3} }\\
\frac1{\sqrt{ 2}}&-\frac1{\sqrt{ 2}} &0\\
\frac1{\sqrt{ 6} }& \frac1{\sqrt{ 6}}   &-\frac2{\sqrt{ 6} }
\end{bmatrix}
\] Show the \(Y_i\)s are iid standard normal. 

\textit{Solution. }  

By theorem 2.2 , \(E[Y] = 0\) and the variance of \(Y\) is 
\begin{align*}
AA'&= \begin{bmatrix}
\frac1{\sqrt{3 }} & \frac1{\sqrt{ 3}}&\frac1{\sqrt{ 3} }\\
\frac1{\sqrt{ 2}}&-\frac1{\sqrt{ 2}} &0\\
\frac1{\sqrt{ 6} }& \frac1{\sqrt{ 6}}   &-\frac2{\sqrt{ 6} }
\end{bmatrix}\begin{bmatrix}
\frac1{\sqrt{3 }} & \frac1{\sqrt{ 3}}&\frac1{\sqrt{ 3} }\\
\frac1{\sqrt{ 2}}&-\frac1{\sqrt{ 2}} &0\\
\frac1{\sqrt{ 6} }& \frac1{\sqrt{ 6}}   &-\frac2{\sqrt{ 6} }
\end{bmatrix}'\\
&=\begin{bmatrix}
\frac1{\sqrt{3 }} & \frac1{\sqrt{ 3}}&\frac1{\sqrt{ 3} }\\
\frac1{\sqrt{ 2}}&-\frac1{\sqrt{ 2}} &0\\
\frac1{\sqrt{ 6} }& \frac1{\sqrt{ 6}}   &-\frac2{\sqrt{ 6} }
\end{bmatrix}\begin{bmatrix}
\frac1{\sqrt{3 }} &\frac1{\sqrt{ 2}}&\frac1{\sqrt{ 6} }\\
 \frac1{\sqrt{ 3}}&-\frac1{\sqrt{ 2}} & \frac1{\sqrt{ 6}}   \\
 \frac1{\sqrt{ 3} }& 0& -\frac2{\sqrt{ 6} }
\end{bmatrix}\\
&=  \begin{bmatrix}
1 & 0 &0 \\
0 & 1&0 \\ 
0 & 0 & 1 
\end{bmatrix}
\end{align*}

\begin{flushright}
\(\blacksquare\)
\end{flushright} 


\newpage
\textbf{Exercises 2c, 3.} 

Let \(Y \sim N_3(\mu,\Sigma) \) with 
\[
\Sigma = \begin{bmatrix}
1& \rho &0\\
\rho & 1 & \rho \\
0 & \rho & 1 
\end{bmatrix}
\]
For what value of \(\rho \) is \(Y_1 + Y_2 +Y_3\) and \(Y_1-Y_2-Y_3\) independent? 


\vspace{4mm} 
\textit{Solution. } By theorem 2.2, \(X=CY\). Variance of \(X\) is, 
\begin{align*}
\Var[X] &= CYC']\\
&= \begin{bmatrix}
2+4\rho & -1-2\rho\\
-1-2\rho & 3 
\end{bmatrix}
\end{align*}
Thus, \(X \) is independent iff \(\rho = 1/2\). 

\begin{flushright}
\(\blacksquare\)
\end{flushright} 

\newpage
\textbf{2d, 3.} 
If \(Y \sim N_2(0, I_2)\) find the values of \(a,b\) such that \[
a(Y_1-Y_2)^2 + b(Y_1+Y_2)^2 \sim \chi^2
\]


\vspace{4mm} 
\textit{Solution. } Using theorem 2.7, we need to express this problem as \( Y'AY \) for an idempotent \(A\). First notice that \((Y_1-Y_2)^2 + (Y_1+Y_2)^2  = X'X \) if \(X=\begin{bmatrix} 1&-1\\1&1\end{bmatrix}Y = BY \). Thus if we let \(A=[a \hspace{3mm} b]\), then \(A'X'X= a(Y_1-Y_2)^2 + b(Y_1+Y_2)^2\).  A quick calculation of the matrices yields, 
\begin{align*} 
a(Y_1-Y_2)^2 + b(Y_1+Y_2)^2 &= Y'\begin{bmatrix} 1&1\\-1&1\end{bmatrix}\begin{bmatrix} a&0\\0&b\end{bmatrix}\begin{bmatrix}1 &-1\\1&1\end{bmatrix}Y\\
&= Y' \begin{bmatrix} a+b&-a+b\\-a+b&a+b\end{bmatrix} Y
\end{align*} 
We have represented the problem with a symmetric matrix \(A\). We now just need to find the values of \(a,b\) \(\ni\) \(A^2=A\). Setting \(A^2=A\), we get
\begin{align*}
A^2 &= 
\begin{bmatrix}
2a^2+2b^2 & -2a^2 + 2b^2 \\
 -2a^2 + 2b^2&2a^2+2b^2 
\end{bmatrix}
\end{align*}


And this equals \(A\) if and only if 
\begin{align*}
2a^2+2b^2 &= a+b  &&\text{and}   &             -2a^2+2b^2&= -a+b  \\
2a^2 &= a+b -2b^2 &&&    -a-b+2b^2 + 2b^2&= -a+b\\
&&&&  4b^2-2b &= 0 \\
2a^2-a &= \frac12-2\cdot \frac14   &&& 2b(b-1)&= 0\\
a(2a-1)&= 0&&& b= \frac12\\
a&= \frac12
\end{align*}

\begin{flushright}
\(\blacksquare\)
\end{flushright} 



\newpage
\textbf{2d 4. }  Suppose that \(Y \sim N_3(0, I_n)\). Show that
\[
\frac13[(Y_1-Y_2)^2 + (Y_2-Y_3)^2 + (Y_3-1)^2]
\]has a \(\chi_2^2 \) distribution.  


 \vspace{3mm}
 
 \textit{Solution. } First notice that we can write \(\frac13[(Y_1-Y_2)^2 + (Y_2-Y_3)^2 + (Y_3-1)^2] \) 
 as 
 \[
 \frac13[(Y_1-Y_2)^2 + (Y_2-Y_3)^2 + (Y_3-1)^2] =
 Y' \left(\begin{array}{ccc}
 \frac{2}{3} & -\frac{1}{3} & -\frac{1}{3} \\
 -\frac{1}{3} & \frac{2}{3} & -\frac{1}{3} \\
 -\frac{1}{3} & -\frac{1}{3} & \frac{2}{3} \\
\end{array}
\right) Y
 \]
Thus, by theorem 2.8, if we show that 2 of the eigenvalues of \(A\) are 1 and one is zero, then we are done. 
Using mathematica, we find, 
\begin{verbatim}
Eigenvalues[{{2/3, -(1/3), -(1/3)}, {-(1/3), 2/
   3, -(1/3)}, {-(1/3), -(1/3), 2/3}}]
   
   {1, 1, 0}
\end{verbatim}

Thus, 
\[
\text{Eigenvalues}\left[\left(
\begin{array}{ccc}
 \frac{2}{3} & -\frac{1}{3} & -\frac{1}{3} \\
 -\frac{1}{3} & \frac{2}{3} & -\frac{1}{3} \\
 -\frac{1}{3} & -\frac{1}{3} & \frac{2}{3} \\
\end{array}
\right)\right] = [1,1,0]
\]




\newpage
\textbf{Chapter 2 Miscellaneous Exercise 3. }
If \(Y_1, ..., Y_n\) is random sample from \(N(\mu, \sigma^2)\), prove \(\bar Y \) is independent of \(\sum_{i=1}^{n-1} (Y_i-Y_{i+1})^2\) .




 \vspace{3mm}
 
 \textit{Solution. } Notice that \(\bar Y= n^{-1}1_n' Y \) and \(\sum_{i=1}^{n-1} (Y_i-Y_{i+1})^2 = (AY)'(AY) \) 
 where
 
 \[A = \begin{bmatrix} 1&-1&0&\cdots&0\\
 0&1&-1&\cdots &0\\
 \vdots&\ddots&\ddots&\ddots & \vdots\\
 0&\cdots&\cdots &1& -1\\
  0&\cdots&\cdots &\cdots& 1\\
 \end{bmatrix}\]
 
 So let \(U =  n^{-1}1_n' Y\) and \(V = AY\) Then 
 \begin{align*}
 Cov[U,V] &= Cov[ n^{-1}1_n' Y,AY]  \\
 &= n^{-1}1_n' Cov[Y,Y] A' \\
 &=\sigma^2\begin{bmatrix}
 1&1&\cdots&1
 \end{bmatrix}
 \begin{bmatrix} 1&0&0&\cdots&0\\
 -1&1&0&\cdots &0\\
 \vdots&\ddots&\ddots&\ddots & \vdots\\
 0&\cdots&-1 &1& 0\\
  0&\cdots&\cdots &-1& 1\\
 \end{bmatrix}\\
 &= \sigma^2 \begin{bmatrix}
 0&0&\cdots&0
 \end{bmatrix}\\
 &= \begin{bmatrix}
 0&0&\cdots&0
 \end{bmatrix}
 \end{align*}
 
 Thus \(U,V\) are independent. 
 
 \vspace{15mm} 
 \textbf{NOTE:} I did not see in the theorem where this connects to showing that \(\bar Y \) is independent from \((AY)'(AY)\). I think this is the case but have not made the connection yet. 
 
 \newpage
 \textbf{Chapter 2 Miscellaneous Exercise 8} 
 Let \(Y \sim N_n(0,I_n) \), let \(A,B\) be symmetric idempotent matrices with \(AB=BA=0\). Show that \(Y'AY, Y'BY\) and \(Y'(I_n -A-B)Y\) have independent chi-square distributions. 
 
 
 
 \vspace{3mm}
 
 \textit{Solution. }  
By theorem 2.7, we get that each of these quadratic forms will have a \(\chi^2\)  distribution. We only have to show independence. Thus, if we let \(U=AY\), \(V=BY\), and \(W=(I_n-A-B)Y\), then

\begin{align*}
Cov(U,V) &=Cov(AY,BY)  & Cov(U,W)&=Cov[AY,(I_n-A-B)Y] & Cov(V,W)&=Cov[BY,(I_n-A-B)Y ]\\
&= A\Var[Y]B' & &= A\Var[Y](I_n-A-B)' & &= B\Var[Y](I_n-A_B)' \\
&= AI_nB' & &= AI_n(I_n'-A'-B') & &=BI_n(I_n'-A'-B')\\
&=AB & &=AI_n - A^2 -AB & &= BI_n - BA - B^2 \\
&= 0 & &= A-A-0 & &= B-BA-B\\
&= 0 &&=0 &&=0
\end{align*}
 
 
 
 \begin{flushright}
\(\blacksquare\)
\end{flushright} 

 
 
 
 
 
 

\end{document} 












